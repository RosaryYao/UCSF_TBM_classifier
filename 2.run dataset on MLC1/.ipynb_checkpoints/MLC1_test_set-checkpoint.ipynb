{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Import the required libraries#### \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import sklearn.metrics as metrics\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####User input the files#### \n",
    "\n",
    "dataframe1 = pd.read_csv('data_files/training_validation.csv').set_index('ensembl') \n",
    "dfb = pd.read_csv('data_files/metadata.csv').set_index('samples')\n",
    "\n",
    "df_TEST = pd.read_csv('data_files/final_test_set.csv').set_index('ensembl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Functions for MLC######\n",
    "\n",
    "def pick_ref_sample(x):\n",
    "    \n",
    "    \"\"\"Pick a reference sample to calculate normalization factors for input sample. Output from this function is fed as input into calcNormFactors_py()\"\"\"\n",
    "    \n",
    "    #Remove genes that are zeros throughout\n",
    "    num = x.shape[1]\n",
    "    headers = x.columns\n",
    "    x['sum'] = x.sum(axis=1)\n",
    "    y = x[~x['sum'].isin([0])]\n",
    "    z = y.iloc[:,0:num].T\n",
    "    xx = z \n",
    "    \n",
    "    #Get cpm\n",
    "    xx['sum'] = xx.sum(axis=1)\n",
    "    zz = (xx.loc[:, xx.columns != \"sum\"].div(xx[\"sum\"], axis=0).T).iloc[:,:(num)]\n",
    "    \n",
    "    #Get 75% quant table\n",
    "    quant_75_table = zz.quantile([.75], axis = 0).T\n",
    "    average_quant_75_table = float(zz.quantile([.75], axis = 0).T.sum()/len(zz.quantile([.75], axis = 0).T))\n",
    "    \n",
    "    #Pick reference sample using edgeR method\n",
    "    \n",
    "    Ref_sample_table = abs(quant_75_table-average_quant_75_table).sort_values(by=0.75)\n",
    "    Ref_sample = Ref_sample_table.index[0]\n",
    "    \n",
    "    \n",
    "    return (zz, Ref_sample)\n",
    "\n",
    "\n",
    "def calcNormFactors_py(zz, Ref_sample, output_scaling_factor_file,dataframe_OG):\n",
    "    \n",
    "    \"\"\"Python implementation of edgeR's calcNormFactors.\"\"\"\n",
    "    \n",
    "    #Calculate log(2) ratio's of reference sample/input sample\n",
    "    Ratio_zz = 1/(zz.loc[:].div(zz[Ref_sample], axis=0))\n",
    "    mat1 = np.log2(Ratio_zz)\n",
    "    \n",
    "    #Array of sample IDs\n",
    "    all_samples = mat1.columns\n",
    "    \n",
    "    textfile = open(output_scaling_factor_file,\"a\")\n",
    "\n",
    "    for i in range(len(all_samples)): \n",
    "        if all_samples[i] == Ref_sample:\n",
    "            #scaling factor for Ref_sample is 1\n",
    "            print (all_samples[i], 1, file=textfile,sep=\",\")\n",
    "        else:\n",
    "            #generate 2 matrices 1) Genes in ref and input samples have finite values, 2) Geometric mean of genes in ref and input samples\n",
    "            mat_t = mat1[[all_samples[i], Ref_sample]]\n",
    "            mat_t1 = mat_t[(~mat_t[all_samples[i]].isin(['NaN', 'inf','-inf'])) & (~mat_t[Ref_sample].isin(['NaN', 'inf','-inf']))]\n",
    "            mat_t1_sort = mat_t1.sort_values(by=all_samples[i])\n",
    "    \n",
    "            t1 = ((mat1[all_samples[i]] + mat1[ Ref_sample] )/ 2).to_frame()\n",
    "            mat_t2 = t1[(~t1[0].isin(['NaN', 'inf','-inf',0]))] #in addition to filtering out genes that are infinity, remove genes that are 0 as well\n",
    "            mat_t2_sort = mat_t2.sort_values(by=0)\n",
    "\n",
    "            #filter out top and bottow 30% genes of mat_t1_sort, and top and bottom 5% genes of mat_t2_sort\n",
    "            num30 = int(np.round(len(mat_t1_sort)*0.30))\n",
    "            num05 = int(np.round(len(mat_t2_sort)*0.05))\n",
    "\n",
    "            keep_after30 = mat_t1_sort.index[num30:(len(mat_t1_sort.index))-num30]\n",
    "            keep_after05 = mat_t2_sort.index[num05:(len(mat_t2_sort.index))-num05]\n",
    "            \n",
    "            #Keep genes common to both keep_after30 and keep_after05 lists\n",
    "            genes_common = np.intersect1d(keep_after30,keep_after05)\n",
    "                        \n",
    "                \n",
    "            #If the intersection between them is zero, drop until it is not\n",
    "            if len(genes_common) == 0:\n",
    "                j = 30\n",
    "                while len(genes_common) == 0:\n",
    "                    num30 = int(np.round(len(mat_t1_sort)*j)/100)\n",
    "                    num05 = int(np.round(len(mat_t2_sort)*0.05))\n",
    "\n",
    "                    keep_after30 = mat_t1_sort.index[num30:(len(mat_t1_sort.index))-num30]\n",
    "                    keep_after05 = mat_t2_sort.index[num05:(len(mat_t2_sort.index))-num05]\n",
    "                    genes_common = np.intersect1d(keep_after30,keep_after05) \n",
    "                    j = j - 1\n",
    "                    if j == 1:\n",
    "                        break\n",
    "                \n",
    "            #calculate scaling factor\n",
    "            log_ratio_sample = mat1[all_samples[i]].loc[genes_common].to_frame()\n",
    "            original_sample = dataframe_OG[all_samples[i]].loc[genes_common].to_frame()\n",
    "           \n",
    "            if len(log_ratio_sample) == 0:\n",
    "                scaling_factor = 0\n",
    "            else:\n",
    "                weighted_avg = float((log_ratio_sample * original_sample).sum())/float(original_sample.sum())\n",
    "                scaling_factor = 2**(-weighted_avg)\n",
    "            print(all_samples[i], scaling_factor, file=textfile,sep=\",\")\n",
    "        \n",
    "    textfile.close()\n",
    "    return \n",
    "\n",
    "\n",
    "def normalize_matrix_scaling_logcpm(df,scaling_factors,prior_count):\n",
    "    \n",
    "    \"\"\"Peform the log CPM transformation using scaling factors from edgeRs method\"\"\"\n",
    "    #Read in input raw count dataframe, and scaling factor CSV file obtained using edgeR method\n",
    "    test = df.T.reset_index()\n",
    "    scaling = pd.read_csv(scaling_factors, header = None)\n",
    "    scaling.drop_duplicates(keep='first', inplace=True)\n",
    "    test_x = test.merge(scaling,how='left', left_on='index', right_on=0).drop(0,axis=1).set_index('index')\n",
    "    \n",
    "    #scale matrix with normlaization factors\n",
    "    new_scale = test_x.loc[:, test_x.columns != 1].div(test_x[1], axis=0)\n",
    "    \n",
    "    #CPM normalization\n",
    "    new_scale['sum'] = test_x.sum(axis=1)\n",
    "    scaled = (new_scale.loc[:, new_scale.columns != \"sum\"].div(new_scale[\"sum\"], axis=0))*1000000\n",
    "    \n",
    "    #log2 transform using user inputted prior count\n",
    "    value_for_CF = np.log2(scaled+prior_count).T\n",
    "\n",
    "    return value_for_CF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####MLC predictions######\n",
    "\n",
    "f = open(\"data_files/test_set_prediction.csv\", \"w\")\n",
    "writer = csv.DictWriter(\n",
    "    f, fieldnames=[\"Sample ID\", \"Predicted value\"])\n",
    "writer.writeheader()\n",
    "f.close()\n",
    "\n",
    "textfile = open(\"data_files/test_set_prediction.csv\",\"a\")\n",
    "\n",
    "BS = 100 #How many times to bootstrap\n",
    "\n",
    "for i in np.arange(BS):\n",
    "    \n",
    "    ####split the dataset into 80% to test your classifier on the current classifier###\n",
    "\n",
    "    n = dataframe1.shape[1]\n",
    "    perm = np.random.permutation(n)\n",
    "    a = int(np.round(0.8 * n))\n",
    "\n",
    "    new_perm = dataframe1.iloc[:,perm]\n",
    "    train_x = new_perm.iloc[:,:a]\n",
    "\n",
    "    \n",
    "    ####perform normalization####\n",
    "    \n",
    "    #pick the reference samples from the training data#\n",
    "    norm_trainx, ref = pick_ref_sample(train_x)\n",
    "    \n",
    "    #calculate scaling factors for the training data#\n",
    "    calcNormFactors_py(norm_trainx, ref, 'scaling_factors.csv', dataframe1)\n",
    "    \n",
    "    #convert validation matrix to a format suitable for calcNormFactors_py function#\n",
    "    test_TrainGenes_x = norm_trainx[ref].to_frame().merge(df_TEST, how='left', left_index = True, right_index = True).drop(ref, axis = 1)\n",
    "    test_TrainGenes = test_TrainGenes_x.merge(new_perm[ref].to_frame(), how='left', left_index = True, right_index = True).T\n",
    "    \n",
    "    test_TrainGenes['sum'] = test_TrainGenes.sum(axis=1)\n",
    "    norm_testx = (test_TrainGenes.loc[:, test_TrainGenes.columns != \"sum\"].div(test_TrainGenes[\"sum\"], axis=0).T)\n",
    "    \n",
    "    #calculate scaling factors for the training data#\n",
    "    calcNormFactors_py(norm_testx, ref, 'scaling_factors.csv', df_TEST)\n",
    "    \n",
    "    #calculate normalized training and validation matices#\n",
    "    new_perm_x = normalize_matrix_scaling_logcpm(new_perm,'scaling_factors.csv',75)\n",
    "    train_norm = new_perm_x.iloc[:,:a].dropna(axis=1)\n",
    "    test_norm = normalize_matrix_scaling_logcpm(df_TEST,'scaling_factors.csv',75).dropna(axis=1)\n",
    "    \n",
    "       \n",
    "    #remove scaling_factors.csv file\n",
    "    os.remove(\"scaling_factors.csv\")\n",
    "    \n",
    "    \n",
    "    ####feature selection####\n",
    "    \n",
    "    #merge the key with the normalized training dataset\n",
    "    train_norm_merged = train_norm.T.merge(dfb, how='left', left_index = True, right_index = True)\n",
    "    train_norm_fit = train_norm_merged.iloc[:,0:-1]\n",
    "    train_norm_fit_key = train_norm_merged.iloc[:,-1]\n",
    "\n",
    "    #perform the chi-square analysis\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=50)\n",
    "    fit = bestfeatures.fit(train_norm_fit,train_norm_fit_key)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(train_norm_fit.columns)\n",
    "    \n",
    "    #concat genes and scores into one dataframe, and pick top 500 features\n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Genes','Score']\n",
    "    s_featureScores = featureScores.sort_values(['Score'], ascending=False)\n",
    "    univariate_features = s_featureScores[:50]['Genes'].values\n",
    "    \n",
    "    #generate the new training and validation dataset based on univariate_features\n",
    "    train_norm_x = train_norm[train_norm.index.isin(univariate_features)]\n",
    "    train_norm_x_merged = train_norm_x.T.merge(dfb, how='left', left_index = True, right_index = True)\n",
    "    \n",
    "\n",
    "    #generate the new training and validation dataset based on univariate_features\n",
    "    train_norm_x = train_norm[train_norm.index.isin(univariate_features)]\n",
    "    train_norm_x_merged = train_norm_x.T.merge(dfb, how='left', left_index = True, right_index = True)\n",
    "    \n",
    "    training = train_norm_x_merged.drop('key',axis=1)\n",
    "    test = test_norm[test_norm.index.isin(univariate_features)].T\n",
    "        \n",
    "    Y = train_norm_x_merged.loc[:,'key'] \n",
    "    \n",
    "    ####SVM####\n",
    "    \n",
    "    lsvc = LinearSVC(C=1/10, penalty=\"l1\", dual=False).fit(training.values,Y.values)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    predicted = lsvc.predict(test.values) #predicting on the remaining samples\n",
    "\n",
    "    idx = test.index\n",
    "    SVMpred = predicted\n",
    "    \n",
    "    for k in range(len(idx)):\n",
    "        print (idx[k], SVMpred[k], file=textfile,sep=\",\")\n",
    "        \n",
    "textfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
